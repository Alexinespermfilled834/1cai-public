"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π ML‚Äë–º–æ–¥–µ–ª–∏ 1C AI Stack.

Usage:
    python scripts/eval/eval_model.py --model ./models/demo-model --questions output/dataset/DEMO_qa.jsonl --limit 10
"""

import argparse
import json
from pathlib import Path
from typing import Iterable, Dict, Any, List


def load_dataset(path: Path, limit: int) -> List[Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as fh:
        lines = fh.readlines()

    samples = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        try:
            samples.append(json.loads(line))
        except json.JSONDecodeError:
            continue
        if 0 < limit <= len(samples):
            break
    return samples


def evaluate(model_path: Path, dataset: List[Dict[str, Any]]) -> None:
    if not model_path.exists():
        raise FileNotFoundError(f"Model path not found: {model_path}")

    print(f"üìÅ Model path: {model_path}")
    print(f"üìä Samples to evaluate: {len(dataset)}")
    print("\n‚ö†Ô∏è  Demo evaluator: –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –Ω–∞–ª–∏—á–∏–µ –æ—Ç–≤–µ—Ç–æ–≤.\n")

    missing_answer = 0
    missing_metadata = 0
    total = len(dataset)

    for sample in dataset:
        answer = sample.get("answer")
        if not answer:
            missing_answer += 1
        metadata = sample.get("metadata")
        if not metadata:
            missing_metadata += 1

    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total}")
    print(f"  ‚Ä¢ –ë–µ–∑ answer: {missing_answer}")
    print(f"  ‚Ä¢ –ë–µ–∑ metadata: {missing_metadata}")

    if total > 0:
        quality_score = ((total - missing_answer) / total) * 100
        print(f"\n–û—Ü–µ–Ω–∫–∞ (—É—Å–ª–æ–≤–Ω–∞—è): {quality_score:.1f}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.")
    else:
        print("\n–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç–æ–π ‚Äî –Ω–∏—á–µ–≥–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å.")


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate 1C AI demo model")
    parser.add_argument("--model", required=True, help="–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–ª–∏ —Ñ–∞–π–ª)")
    parser.add_argument("--questions", required=True, help="JSONL —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏/–æ—Ç–≤–µ—Ç–∞–º–∏")
    parser.add_argument("--limit", type=int, default=20, help="–°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å")

    args = parser.parse_args()
    model_path = Path(args.model)
    dataset_path = Path(args.questions)

    dataset = load_dataset(dataset_path, args.limit)
    evaluate(model_path, dataset)


if __name__ == "__main__":
    main()

